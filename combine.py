import os
import sys
import pickle
import numpy as np
import re
import json
import pandas as pd
from sklearn.model_selection import train_test_split
from keras.callbacks import ModelCheckpoint
from keras.models import Sequential, load_model
from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout, Activation
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix
from keras.callbacks import Callback
import tensorflow as tf
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch_geometric.data
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, global_add_pool
from torch.nn import Linear
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix
from sentence_transformers import SentenceTransformer
from torch_geometric.nn import GINConv
from torch_geometric.data import Batch
import torch_geometric.nn
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import AdaBoostClassifier

# Import các hàm xử lý dữ liệu và mô hình đã tải
from model_gin import GIN  
import tensorflow as tf

# Đường dẫn tới các file mô hình đã lưu
model_gin_path = "model_gin.pth"

def convert_to_data(nodes, label, sources, targets):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    if len(nodes) == 0:
        print("No nodes found in the input data. Skipping...")
        return None

    # Node Embedding
    x = torch.tensor(np.array([model.encode(node) for node in nodes]), dtype=torch.float)

    # Node Mapping
    node_mapping = {node: i for i, node in enumerate(nodes)}
    sources_mapped = [node_mapping[source] for source in sources]
    targets_mapped = [node_mapping[target] for target in targets]

    edge_index = torch.tensor([sources_mapped, targets_mapped], dtype=torch.long)

    y = torch.tensor(label, dtype=torch.long)
    return Data(x=x, edge_index=edge_index, y=y, batch=None)

def read_result_file(result_file_path):
    nodes = []
    edges = []
    source = []
    target = []

    with open(result_file_path, 'r') as result_file:
        lines = result_file.readlines()

        # Flag to determine if the current line is for nodes or edges
        is_node_info = False

        for line in lines:
            line = line.strip()

            # Check if the line contains node information
            if line.startswith("Node Address"):
                # Use a regular expression to extract the node value
                match = re.search(r'Node Address: <CFGENode (.*?)>, Node Data:', line)
                if match:
                    node_info_str = match.group(1)
                    nodes.append(node_info_str)
                    #print(f'Nodes:{node_info_str}')
                    
            # Check if the line contains edge information
            elif line.startswith("Edge"):
                # Use a regular expression to extract the edge values
                match = re.search(r'Edge: <CFGENode (.*?)> -> <CFGENode (.*?)>, Edge Data: (.*)', line)
                if match:
                    source_node = match.group(1)
                    target_node = match.group(2)
                    edge_data_str = match.group(3)
                    # Handle the case where the edge data is not properly formatted as JSON
                    try:
                        edge_data = json.loads(edge_data_str)
                    except json.JSONDecodeError:
                        # Handle the case where the edge data is not properly formatted as JSON
                        # You may want to modify this part based on the actual format of your data
                        edge_data = {"raw_data": edge_data_str}
                    #print(source_node)
                    #print(target_node)
                    edges.append((source_node, target_node, edge_data))
                    source.append(source_node)
                    target.append(target_node)

    return nodes, edges, source, target

def predict_with_gin(gin_file, filetype):
    # Tải Mô Hình GIN
    model_gin = GIN()
    model_gin.load_state_dict(torch.load(model_gin_path))
    test_data = []
    nodes, edges, sources, targets = read_result_file(gin_file)

    # Đọc dữ liệu từ file, xử lý và chuyển đổi thành định dạng phù hợp với mô hình GIN
    data = convert_to_data(nodes, filetype, sources, targets)
    
    if data is not None:
        test_data.append(data)
            
    test_loader = torch_geometric.data.DataLoader(test_data, batch_size = 32, shuffle=False)
    output_GIN = []

    model_gin.eval()
    test_predictions = []
    test_targets = []

    with torch.no_grad():
        for data in test_loader:
            #print("Evaluating model for graph with", len(data.x), "nodes")
            output = model_gin(data.x, data.edge_index, data.batch)
            #print(output.numpy())
            output = torch.tensor(output, dtype=torch.float32)
            output_GIN.append(output)
            predictions = output.argmax(dim=1)
            test_predictions.extend(predictions.cpu().numpy())
            test_targets.extend(data.y.cpu().numpy())
            
    accuracy = accuracy_score(test_targets, test_predictions)
    recall = recall_score(test_targets, test_predictions)
    f1 = f1_score(test_targets, test_predictions)

    print(f'Test Accuracy for GIN: {accuracy:.4f}')
    print(f'Test Recall for GIN: {recall:.4f}')
    print(f'Test F1 Score for GIN: {f1:.4f}')

    output_GIN_tensor = output_GIN[0]
    for i in range(1, len(output_GIN)):
        output_GIN_tensor = torch.cat((output_GIN_tensor, output_GIN[i]), dim=0)

    return output_GIN_tensor, test_targets


# # Thực hiện dự đoán với mô hình GIN
# predictions_gin = model_gin(gin_data_path)

class MetricsCallback(Callback):
    def __init__(self, X_val, y_val):
        super(MetricsCallback, self).__init__()
        self.X_val = X_val
        self.y_val = y_val

    def on_epoch_end(self, epoch, logs=None):
        y_pred = self.model.predict(self.X_val)
        y_pred_classes = np.argmax(y_pred, axis=1)
        y_true = np.argmax(self.y_val, axis=1)

        accuracy = accuracy_score(y_true, y_pred_classes)
        precision = precision_score(y_true, y_pred_classes)
        recall = recall_score(y_true, y_pred_classes)
        f1 = f1_score(y_true, y_pred_classes)

        print(f'Test Accuracy for CNN: {accuracy:.4f}')
        print(f'Test Recall for CNN: {recall:.4f}')
        print(f'Test precision for CNN: {precision:.4f}')
        print(f'Test F1 Score for CNN: {f1:.4f}')

class MultimodalFunction(nn.Module):
    def __init__(self, input_size):
        super(MultimodalFunction, self).__init__()
        
        # Define layers for processing multimodal inputs
        self.fc1 = nn.Linear(4, 2)
        
        self.fc2 = nn.Linear(2, 1)
        
        # Define activation functions
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()  # Sử dụng hàm sigmoid
        
    def forward(self, input_tensor):
        # Process multimodal input
        x = self.relu(self.fc1(input_tensor))
        x = self.fc2(x)
        
        output = self.sigmoid(x)
        return output

def model_CNN_(file):
    df = pd.read_csv(file)
    X = df.drop(['FileName','MD5','Machine','Type of file PE','Type'],axis=1).values
    y = df['Type'].values

    X = np.array(df.drop(['FileName','MD5','Machine','Type of file PE','Type'], axis=1).values, dtype=np.float32)
    y = np.array(df['Type'].values, dtype=np.float32)
    # label ban đầu: y[:, 1]
    y = np.array([np.absolute(y - 1), y], dtype=np.int32).T
    m, n = X.shape
    
    max_min_data = []
    for i in range(n):
        max_Xi = float(max(X[:, i]))
        min_Xi = float(min(X[:, i]))

        if len(max_min_data) == i:
            max_min_data.append([max_Xi, min_Xi])

        if max_Xi > max_min_data[i][0]:
            max_min_data[i][0] = max_Xi
        else:
            max_Xi = max_min_data[i][0]

        if min_Xi < max_min_data[i][1]:
            max_min_data[i][1] = min_Xi
        else:
            min_Xi = max_min_data[i][1]

        if max_Xi != min_Xi:
            X[:, i] = (X[:, i] - min_Xi) / (max_Xi - min_Xi)
        else:
            X[:, i] = 0

    X = X.reshape(m, n, 1)
    batch_size = 512
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.009, shuffle=False)
    #print("OF CNN")
    print(len(X_train), len(X_test), len(y_train), len(y_test))
    
    model_CNN = Sequential()
    model_CNN.add(Conv1D(64, 3, padding='same', input_shape=(X_train.shape[1:]), activation='relu'))
    model_CNN.add(Conv1D(64, 3, activation='relu'))
    model_CNN.add(MaxPooling1D(pool_size=2))
    model_CNN.add(Dropout(0.25))
    model_CNN.add(Conv1D(32, 3, padding='same', activation='relu'))
    model_CNN.add(Conv1D(32, 3, activation='relu'))
    model_CNN.add(MaxPooling1D(pool_size=2))
    model_CNN.add(Dropout(0.25))
    model_CNN.add(Flatten())
    model_CNN.add(Dense(512, activation='sigmoid', kernel_initializer='normal'))
    model_CNN.add(Dense(512, activation='sigmoid', kernel_initializer='normal'))
    model_CNN.add(Dense(512, activation='sigmoid', kernel_initializer='normal'))
    model_CNN.add(Dropout(0.25))
    model_CNN.add(Dense(2, activation='sigmoid', kernel_initializer='normal'))
    model_CNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])

    training_epochs = 100
    model_CNN.fit(X_train, y_train, epochs=int(training_epochs), shuffle=False, batch_size=batch_size, verbose=1)
    '''
    print("labels train of CNN")
    print(y_train[:, 1])
    '''
    output_CNN = model_CNN.predict(X_test)
    output_CNN_tensor = torch.tensor(output_CNN, dtype=torch.float32)
    
    return output_CNN_tensor, np.array(y_test[:, 1])

import torch
import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from combine import model_CNN_, predict_with_gin, MultimodalFunction

def predict(csv_file, graph_file, filetype):
    df1 = pd.read_csv('train.csv')
    df2 = pd.read_csv('output_dataset_header.csv')
    
    # Nối hai DataFrame theo chiều dọc (axis=0)
    concatenated_df = pd.concat([df1, df2], ignore_index=True)

    # Lưu DataFrame đã nối vào một tệp CSV mới
    concatenated_df.to_csv('dataset.csv', index=False)

    # Gọi hàm model_CNN_ để tạo và train model CNN
    output_CNN_tensor, labels_CNN = model_CNN_('dataset.csv')

    # Gọi hàm predict_with_gin để dự đoán với mô hình GIN
    output_GIN_tensor, test_targets = predict_with_gin(graph_file, filetype)
    
    # Chuyển test_targets thành numpy array
    test_targets = np.array(test_targets)
    
    combined_predictions = torch.cat((output_GIN_tensor, output_CNN_tensor), dim=1)
    model = MultimodalFunction(tf.size(combined_predictions))
    output = model(combined_predictions)
    true_output = labels_CNN
    
    min_value = torch.min(output).item()
    threshold = min_value
    temp_output = (output > threshold).type(torch.float32)
    
    # Chuyển đầu ra thành dạng numpy để tính toán các độ đo
    predictions = temp_output.detach().numpy()
    true_output = true_output  # Chuyển đổi tensor thành numpy array

    # Chuyển đổi đầu ra của mô hình CNN thành nhãn phân loại nhị phân
    output_CNN_binary = (output_CNN_tensor.detach().numpy() > 0.5).astype(int)

    # Chuyển đổi đầu ra của mô hình GIN thành nhãn phân loại nhị phân
    output_GIN_binary = (output_GIN_tensor.detach().numpy() > 0.5).astype(int)

    # Chuyển đổi các dự đoán kết hợp thành nhãn phân loại nhị phân nếu cần
    combined_binary_predictions = (predictions > 0.5).astype(int)
    
    cnn_prediction = output_CNN_tensor
    gin_prediction = output_GIN_tensor
    combined_prediction = output

    return {
        'cnn_prediction': cnn_prediction,  # Thêm vào kết quả dự đoán cho CNN
        'gin_prediction': gin_prediction,  # Thêm vào kết quả dự đoán cho GIN
        'combined_prediction': combined_prediction  # Thêm vào kết quả dự đoán cho Combined
    }
