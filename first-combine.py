import os
import sys
import pickle
import numpy as np
import re
import json
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout, Activation
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix
from tensorflow.keras.callbacks import Callback
import tensorflow as tf
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch_geometric.data
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, global_add_pool
from torch.nn import Linear
from sklearn.metrics import accuracy_score, recall_score, f1_score, roc_auc_score
from sentence_transformers import SentenceTransformer
from torch_geometric.nn import GINConv
from torch_geometric.data import Batch
import torch_geometric.nn
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import AdaBoostClassifier

# Define the file path for log.txt
log_file_path = "log.txt"

# Redirect stdout and stderr to log file
log_file = open(log_file_path, "w", encoding="utf-8")
original_stdout = sys.stdout
sys.stdout = log_file

model = SentenceTransformer('all-MiniLM-L6-v2')
count_nodes = set()
Id_node = dict()

def read_result_file(result_file_path):
    nodes = []
    edges = []
    source = []
    target = []

    with open(result_file_path, 'r') as result_file:
        lines = result_file.readlines()

        # Flag to determine if the current line is for nodes or edges
        is_node_info = False

        for line in lines:
            line = line.strip()

            # Check if the line contains node information
            if line.startswith("Node Address"):
                # Use a regular expression to extract the node value
                match = re.search(r'Node Address: <CFGENode (.*?)>, Node Data:', line)
                if match:
                    node_info_str = match.group(1)
                    nodes.append(node_info_str)
                    count_nodes.add(node_info_str)
                    #print(f'Nodes:{node_info_str}')
                    
            # Check if the line contains edge information
            elif line.startswith("Edge"):
                # Use a regular expression to extract the edge values
                match = re.search(r'Edge: <CFGENode (.*?)> -> <CFGENode (.*?)>, Edge Data: (.*)', line)
                if match:
                    source_node = match.group(1)
                    target_node = match.group(2)
                    edge_data_str = match.group(3)
                    # Handle the case where the edge data is not properly formatted as JSON
                    try:
                        edge_data = json.loads(edge_data_str)
                    except json.JSONDecodeError:
                        # Handle the case where the edge data is not properly formatted as JSON
                        # You may want to modify this part based on the actual format of your data
                        edge_data = {"raw_data": edge_data_str}
                    #print(source_node)
                    #print(target_node)
                    edges.append((source_node, target_node, edge_data))
                    source.append(source_node)
                    target.append(target_node)

    return nodes, edges, source, target

def create_node_mapping():
    count = 0
    for node in count_nodes:
        Id_node[node] = count+1
        count = count + 1
    
def get_node_id(node_name):
    if node_name not in Id_node:
        print(node_name)
        return -1
    return Id_node[node_name]

def convert_to_data(nodes, label, sources, targets):
    if len(nodes) == 0:
        print("No nodes found in the input data. Skipping...")
        return None

    # Node Embedding
    x = torch.tensor(np.array([model.encode(node) for node in nodes]), dtype=torch.float)

    # Node Mapping
    node_mapping = {node: i for i, node in enumerate(nodes)}
    sources_mapped = [node_mapping[source] for source in sources]
    targets_mapped = [node_mapping[target] for target in targets]

    edge_index = torch.tensor([sources_mapped, targets_mapped], dtype=torch.long)

    y = torch.tensor(label, dtype=torch.long)
    return Data(x=x, edge_index=edge_index, y=y, batch=None)

class GINConv(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super(GINConv, self).__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_channels, out_channels),
            nn.BatchNorm1d(out_channels),
            nn.ReLU(),
            nn.Linear(out_channels, out_channels),
            nn.BatchNorm1d(out_channels)
        )

    def forward(self, x, edge_index):
        return self.mlp(x)

class GIN(nn.Module):
    def __init__(self):
        super(GIN, self).__init__()
        self.conv1 = GINConv(in_channels=384, out_channels=128)
        self.conv2 = GINConv(in_channels=128, out_channels=256)
        self.conv3 = GINConv(in_channels=256, out_channels=512)
        self.lin = nn.Linear(512, 2)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x, edge_index, batch):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.conv3(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = torch_geometric.nn.global_add_pool(x, batch)
        x = self.lin(x)
        return F.log_softmax(x, dim=1)

def model_GIN_(folder_file_1,folder_file_2):
    all_data = []
    train_data = []
    test_data = []
    file_names = os.listdir(folder_file_1)
    file_names.sort(key=lambda x: int(x.split('_')[0].split('.')[0]))
    for filename in file_names:
        pe_file_path = os.path.join(folder_file_1, filename)
        nodes, edges, sources, targets = read_result_file(pe_file_path)
        first_letter_after_underscore = filename.split('_')[2][0]
        if first_letter_after_underscore == 'b':
           print(filename + " -->  benign")
           data = convert_to_data(nodes, 0, sources, targets)  # Label 0 for benign
        else:
           print(filename + " -->  malware")
           data = convert_to_data(nodes, 1, sources, targets)  # Label 1 for malware 
        if data is not None:
            all_data.append(data)
            
    file_names = os.listdir(folder_file_2)
    file_names.sort(key=lambda x: int(x.split('_')[0].split('.')[0]))
    for filename in file_names:
        pe_file_path = os.path.join(folder_file_2, filename)
        nodes, edges, sources, targets = read_result_file(pe_file_path)
        first_letter_after_underscore = filename.split('_')[2][0]
        if first_letter_after_underscore == 'b':
           print(filename + " -->  benign")
           data = convert_to_data(nodes, 0, sources, targets)  # Label 0 for benign
        else:
           print(filename + " -->  malware")
           data = convert_to_data(nodes, 1, sources, targets)  # Label 1 for malware
        if data is not None:
            all_data.append(data)      
    create_node_mapping()
    train_data, test_data = train_test_split(all_data, test_size=0.3, shuffle=False)
    print(len(train_data), len(test_data))
    ################### Train ##########################################################
    model_GIN = GIN()
    optimizer = torch.optim.Adam(model_GIN.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()

    train_loader_gin = torch_geometric.data.DataLoader(train_data, batch_size = 32, shuffle=True)
    
    print("label train of gin")
    for data in train_loader_gin:
        print(data.y.cpu().numpy())
    
    sum_loss = 0
    epoch = 100
    for epoch in range(100):
        model_GIN.train()
        epoch_loss = 0.0
        num_batches = 0
        #print("Epoch:", epoch)
        for data in train_loader_gin:
            print("Training model for graph with", len(data.x), "nodes")
            optimizer.zero_grad()
            output = model_GIN(data.x, data.edge_index, data.batch)
            loss = criterion(output, data.y)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
            sum_loss=epoch_loss
            num_batches += 1
        average_epoch_loss = epoch_loss / num_batches
        print(f'Epoch: {epoch}, Average Loss: {average_epoch_loss}')
    print(f'Average loss when train by GIN: {sum_loss/100}')
    ################### Test ##########################################################
            
    test_loader = torch_geometric.data.DataLoader(test_data, batch_size = 32, shuffle=False)
    return model_GIN, test_loader


class MetricsCallback(Callback):
    def __init__(self, X_val, y_val):
        super(MetricsCallback, self).__init__()
        self.X_val = X_val
        self.y_val = y_val

    def on_epoch_end(self, epoch, logs=None):
        y_pred = self.model.predict(self.X_val)
        y_pred_classes = np.argmax(y_pred, axis=1)
        y_true = np.argmax(self.y_val, axis=1)

        accuracy = accuracy_score(y_true, y_pred_classes)
        precision = precision_score(y_true, y_pred_classes)
        recall = recall_score(y_true, y_pred_classes)
        f1 = f1_score(y_true, y_pred_classes)

        print(f'Test Accuracy for CNN: {accuracy:.4f}')
        print(f'Test Recall for CNN: {recall:.4f}')
        print(f'Test precision for CNN: {precision:.4f}')
        print(f'Test F1 Score for CNN: {f1:.4f}')

def model_CNN_(file):
    df = pd.read_csv(file)
    X = df.drop(['FileName','MD5','Machine','Type of file PE','Type'],axis=1).values
    y = df['Type'].values

    X = np.array(df.drop(['FileName','MD5','Machine','Type of file PE','Type'], axis=1).values, dtype=np.float32)
    y = np.array(df['Type'].values, dtype=np.float32)
    # label ban đầu: y[:, 1]
    y = np.array([np.absolute(y - 1), y], dtype=np.int32).T
    m, n = X.shape
    
    max_min_data = []
    for i in range(n):
        max_Xi = float(max(X[:, i]))
        min_Xi = float(min(X[:, i]))

        if len(max_min_data) == i:
            max_min_data.append([max_Xi, min_Xi])

        if max_Xi > max_min_data[i][0]:
            max_min_data[i][0] = max_Xi
        else:
            max_Xi = max_min_data[i][0]

        if min_Xi < max_min_data[i][1]:
            max_min_data[i][1] = min_Xi
        else:
            min_Xi = max_min_data[i][1]

        if max_Xi != min_Xi:
            X[:, i] = (X[:, i] - min_Xi) / (max_Xi - min_Xi)
        else:
            X[:, i] = 0

    X = X.reshape(m, n, 1)
    batch_size = 512
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, shuffle=False)
    #print("OF CNN")
    #print(len(X_train), len(X_test), len(y_train), len(y_test))
    
    model_CNN = Sequential()
    model_CNN.add(Conv1D(64, 3, padding='same', input_shape=(X_train.shape[1:]), activation='relu'))
    model_CNN.add(Conv1D(64, 3, activation='relu'))
    model_CNN.add(MaxPooling1D(pool_size=2))
    model_CNN.add(Dropout(0.25))
    model_CNN.add(Conv1D(32, 3, padding='same', activation='relu'))
    model_CNN.add(Conv1D(32, 3, activation='relu'))
    model_CNN.add(MaxPooling1D(pool_size=2))
    model_CNN.add(Dropout(0.25))
    model_CNN.add(Flatten())
    model_CNN.add(Dense(512, activation='sigmoid', kernel_initializer='normal'))
    model_CNN.add(Dense(512, activation='sigmoid', kernel_initializer='normal'))
    model_CNN.add(Dense(512, activation='sigmoid', kernel_initializer='normal'))
    model_CNN.add(Dropout(0.25))
    model_CNN.add(Dense(2, activation='sigmoid', kernel_initializer='normal'))
    model_CNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])

    training_epochs = 100
    model_CNN.fit(X_train, y_train, epochs=int(training_epochs), shuffle=False, batch_size=batch_size, verbose=1)
    '''
    print("labels train of CNN")
    print(y_train[:, 1])
    '''
    return model_CNN, X_test, np.array(y_test[:, 1])

class MultimodalFunction(nn.Module):
    def __init__(self, input_size):
        super(MultimodalFunction, self).__init__()
        
        # Define layers for processing multimodal inputs
        self.fc1 = nn.Linear(4, 2)
        
        self.fc2 = nn.Linear(2, 1)
        
        # Define activation functions
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()  # Sử dụng hàm sigmoid
        
    def forward(self, input_tensor):
        # Process multimodal input
        x = self.relu(self.fc1(input_tensor))
        x = self.fc2(x)
        
        output = self.sigmoid(x)
        return output
    
def process_output(model_GIN, test_loader_GIN,model_CNN, X_test_CNN, labels_CNN):
    output_GIN = []
    metrics_callback = MetricsCallback(X_test_CNN, labels_CNN)
    callbacks_list = [metrics_callback]
    output_CNN = model_CNN.predict(X_test_CNN)
    
    # Chuyển đổi dự đoán thành nhãn dự đoán (0 hoặc 1)
    predicted_labels = np.argmax(output_CNN, axis=1)
    label =  np.array(labels_CNN)

    # Tính toán các metric
    accuracy = accuracy_score(label, predicted_labels)
    precision = precision_score(label, predicted_labels)
    recall = recall_score(label, predicted_labels)
    f1 = f1_score(label, predicted_labels)
    
    print("Accuracy for CNN:", accuracy)
    print("Precision for CNN:", precision)
    print("Recall for CNN:", recall)
    print("F1-score for CNN:", f1)

    model_GIN.eval()
    test_predictions = []
    test_targets = []

    with torch.no_grad():
        for data in test_loader_GIN:
            #print("Evaluating model for graph with", len(data.x), "nodes")
            output = model_GIN(data.x, data.edge_index, data.batch)
            #print(output.numpy())
            output = torch.tensor(output, dtype=torch.float32)
            output_GIN.append(output)
            predictions = output.argmax(dim=1)
            test_predictions.extend(predictions.cpu().numpy())
            test_targets.extend(data.y.cpu().numpy())
            
    accuracy = accuracy_score(test_targets, test_predictions)
    precision = precision_score(test_targets, test_predictions)
    recall = recall_score(test_targets, test_predictions)
    f1 = f1_score(test_targets, test_predictions)

    print(f'Test Accuracy for GIN: {accuracy:.4f}')
    print(f'Test Precision for GIN: {precision:.4f}')
    print(f'Test Recall for GIN: {recall:.4f}')
    print(f'Test F1 Score for GIN: {f1:.4f}')

    output_GIN_tensor = output_GIN[0]
    for i in range(1, len(output_GIN)):
        output_GIN_tensor = torch.cat((output_GIN_tensor, output_GIN[i]), dim=0)    
    output_CNN_tensor = torch.tensor(output_CNN, dtype=torch.float32)
    #print("Kích thước của output_GIN_tensor:", tf.size(output_GIN_tensor))
    #print(output_GIN_tensor)
    #print("Kích thước của output_CNN_tensor:", tf.size(output_CNN_tensor))
    #print(output_CNN_tensor)
    # Concatenate the tensors along dimension 1
    combined_predictions = torch.cat((output_GIN_tensor, output_CNN_tensor), dim=1)

    #print("size sau khi combine")
    #print(tf.size(combined_predictions))
    model = MultimodalFunction(tf.size(combined_predictions))
    output = model(combined_predictions)
    print("Original output")
    print(output)
    print(" ############################################ ")
    print("Round")
    # Dự đoán với mô hình
    min_value = torch.min(output).item()
    threshold = min_value
    output = (output <= threshold).type(torch.float32)
    print(output)
    # Chuyển đầu ra thành dạng numpy để tính toán các độ đo
    predictions = output
    #print("Kích thước của combine all:", tf.size(predictions))
    
    accuracy = accuracy_score(test_targets, predictions)
    precision = precision_score(test_targets, predictions)
    recall = recall_score(test_targets, predictions)
    f1 = f1_score(test_targets, predictions)

    print(f'Test Accuracy for All: {accuracy:.4f}')
    print(f'Test Precision for All: {precision:.4f}')
    print(f'Test Recall for All: {recall:.4f}')
    print(f'Test F1 Score for All: {f1:.4f}') 
    
    conf_matrix = confusion_matrix(test_targets, output)
    print("Confusion Matrix:")
    print(conf_matrix)

    # Trích xuất các giá trị TP, TN, FP, FN từ ma trận confusion
    tn, fp, fn, tp = conf_matrix.ravel()

    # In các giá trị
    print(f'True Negative: {tn}')
    print(f'False Positive: {fp}')
    print(f'False Negative: {fn}')
    print(f'True Positive: {tp}')

def save_model_GIN(model, filepath):
    torch.save(model.state_dict(), filepath)
    
def save_model_CNN(model, filepath):
    model.save(filepath)

if __name__ == "__main__":
    print("Doing with GIN !!!")

    folder_file_1 = 'D:\\Server1\\1'
    folder_file_2 = 'D:\\Server1\\2'
    

    model_GIN_1, test_loader_GIN = model_GIN_(folder_file_1,folder_file_2)
    # return model_GIN, test_loader
    print("Doing with CNN !!!")

    model_CNN_2, X_test_CNN, labels_CNN = model_CNN_("combined_output.csv")
    

    # return model_CNN, X_test, np.array(y_test[:, 1])

    print("label test of CNN")
    print(labels_CNN)
    print()
    print("label test of GIN")
    for data in test_loader_GIN:
        print(data.y.cpu().numpy())
        print()
    
    process_output(model_GIN_1, test_loader_GIN, model_CNN_2, X_test_CNN, labels_CNN)
    sys.stdout = original_stdout
    log_file.close()
    
    # Save the GIN model
    model_save_path_gin = "model_gin.pth"
    try:
        save_model_GIN(model_GIN_1, model_save_path_gin)
        print(f"GIN model saved to {model_save_path_gin}")
    except Exception as e:
        print("Error saving GIN model:", e)

    # Save the CNN model
    model_save_path_cnn = "model_cnn.h5"
    try:
        save_model_CNN(model_CNN_2, model_save_path_cnn)
        print(f"CNN model saved to {model_save_path_cnn}")
    except Exception as e:
        print("Error saving CNN model:", e)
    
    model_CNN_2.save_weights("model_cnn_weights.weights.h5")  # Lưu trọng số của mô hình
    
    # Lưu model GIN
    torch.save(model_GIN_1.state_dict(), 'model_GIN.pt')
    # Lưu model CNN
    model_CNN_2.save('model_CNN.h5')
