import os
import sys
import pickle
import numpy as np
import re
import json
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout, Activation
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix
from tensorflow.keras.callbacks import Callback
import tensorflow as tf
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch_geometric.data
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, global_add_pool
from torch.nn import Linear
from sklearn.metrics import accuracy_score, recall_score, f1_score, roc_auc_score
from sentence_transformers import SentenceTransformer
from torch_geometric.nn import GINConv
from torch_geometric.data import Batch
import torch_geometric.nn
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import AdaBoostClassifier

# Define the file path for log.txt
log_file_path = "log.txt"

# Redirect stdout and stderr to log file
log_file = open(log_file_path, "w", encoding="utf-8")
original_stdout = sys.stdout
sys.stdout = log_file

model = SentenceTransformer('all-MiniLM-L6-v2')
count_nodes = set()
Id_node = dict()

def read_result_file(result_file_path):
    nodes = []
    edges = []
    source = []
    target = []

    with open(result_file_path, 'r') as result_file:
        lines = result_file.readlines()

        # Flag to determine if the current line is for nodes or edges
        is_node_info = False

        for line in lines:
            line = line.strip()

            # Check if the line contains node information
            if line.startswith("Node Address"):
                # Use a regular expression to extract the node value
                match = re.search(r'Node Address: <CFGENode (.*?)>, Node Data:', line)
                if match:
                    node_info_str = match.group(1)
                    nodes.append(node_info_str)
                    count_nodes.add(node_info_str)
                    #print(f'Nodes:{node_info_str}')
                    
            # Check if the line contains edge information
            elif line.startswith("Edge"):
                # Use a regular expression to extract the edge values
                match = re.search(r'Edge: <CFGENode (.*?)> -> <CFGENode (.*?)>, Edge Data: (.*)', line)
                if match:
                    source_node = match.group(1)
                    target_node = match.group(2)
                    edge_data_str = match.group(3)
                    # Handle the case where the edge data is not properly formatted as JSON
                    try:
                        edge_data = json.loads(edge_data_str)
                    except json.JSONDecodeError:
                        # Handle the case where the edge data is not properly formatted as JSON
                        # You may want to modify this part based on the actual format of your data
                        edge_data = {"raw_data": edge_data_str}
                    #print(source_node)
                    #print(target_node)
                    edges.append((source_node, target_node, edge_data))
                    source.append(source_node)
                    target.append(target_node)

    return nodes, edges, source, target

def create_node_mapping():
    count = 0
    for node in count_nodes:
        Id_node[node] = count+1
        count = count + 1
    
def get_node_id(node_name):
    if node_name not in Id_node:
        print(node_name)
        return -1
    return Id_node[node_name]

def convert_to_data(nodes, label, sources, targets):
    if len(nodes) == 0:
        print("No nodes found in the input data. Skipping...")
        return None

    # Node Embedding
    x = torch.tensor(np.array([model.encode(node) for node in nodes]), dtype=torch.float)

    # Node Mapping
    node_mapping = {node: i for i, node in enumerate(nodes)}
    sources_mapped = [node_mapping[source] for source in sources]
    targets_mapped = [node_mapping[target] for target in targets]

    edge_index = torch.tensor([sources_mapped, targets_mapped], dtype=torch.long)

    y = torch.tensor(label, dtype=torch.long)
    return Data(x=x, edge_index=edge_index, y=y, batch=None)

class GINConv(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super(GINConv, self).__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_channels, out_channels),
            nn.BatchNorm1d(out_channels),
            nn.ReLU(),
            nn.Linear(out_channels, out_channels),
            nn.BatchNorm1d(out_channels)
        )

    def forward(self, x, edge_index):
        return self.mlp(x)

class GIN(nn.Module):
    def __init__(self):
        super(GIN, self).__init__()
        self.conv1 = GINConv(in_channels=384, out_channels=128)
        self.conv2 = GINConv(in_channels=128, out_channels=256)
        self.conv3 = GINConv(in_channels=256, out_channels=512)
        self.lin = nn.Linear(512, 2)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x, edge_index, batch):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.conv3(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = torch_geometric.nn.global_add_pool(x, batch)
        x = self.lin(x)
        return F.log_softmax(x, dim=1)

def model_GIN_(folder_file_1):
    all_data = []
    train_data = []
    test_data = []
    file_names = os.listdir(folder_file_1)
    file_names.sort(key=lambda x: int(x.split('_')[0].split('.')[0]))
    for filename in file_names:
        pe_file_path = os.path.join(folder_file_1, filename)
        nodes, edges, sources, targets = read_result_file(pe_file_path)
        first_letter_after_underscore = filename.split('_')[2][0]
        if first_letter_after_underscore == 'b':
           print(filename + " -->  benign")
           data = convert_to_data(nodes, 0, sources, targets)  # Label 0 for benign
        else:
           print(filename + " -->  malware")
           data = convert_to_data(nodes, 1, sources, targets)  # Label 1 for malware 
        if data is not None:
            all_data.append(data)
                 
    create_node_mapping()
    train_data = all_data
    ################### Train ##########################################################
    model_GIN = GIN()
    optimizer = torch.optim.Adam(model_GIN.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()

    train_loader_gin = torch_geometric.data.DataLoader(train_data, batch_size = 32, shuffle=True)
    
    print("label train of gin")
    for data in train_loader_gin:
        print(data.y.cpu().numpy())
    
    sum_loss = 0
    epoch = 100
    for epoch in range(100):
        model_GIN.train()
        epoch_loss = 0.0
        num_batches = 0
        #print("Epoch:", epoch)
        for data in train_loader_gin:
            print("Training model for graph with", len(data.x), "nodes")
            optimizer.zero_grad()
            output = model_GIN(data.x, data.edge_index, data.batch)
            loss = criterion(output, data.y)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
            sum_loss=epoch_loss
            num_batches += 1
        average_epoch_loss = epoch_loss / num_batches
        print(f'Epoch: {epoch}, Average Loss: {average_epoch_loss}')
    print(f'Average loss when train by GIN: {sum_loss/100}')
    ################### Test #########################################################
    return model_GIN

def save_model_GIN(model, filepath):
    torch.save(model.state_dict(), filepath)

if __name__ == "__main__":
    print("Doing with GIN !!!")

    folder_file_1 = 'D:\\Server6\\BongDelayzzz\\1'
    

    model_GIN_1 = model_GIN_(folder_file_1)
    # return model_GIN, test_loader
    print("Doing with CNN !!!")
    
    # Save the GIN model
    model_save_path_gin = "model_gin.pth"
    try:
        save_model_GIN(model_GIN_1, model_save_path_gin)
        print(f"GIN model saved to {model_save_path_gin}")
    except Exception as e:
        print("Error saving GIN model:", e)
